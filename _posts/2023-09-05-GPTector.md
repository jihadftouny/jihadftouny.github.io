---
layout: post
title: "GPTector: An experiment with detecting short text generated by Open AI's ChatGPT"
categories: misc
---

Hey there! Since this is my first blog post, let me start by telling you a bit about myself.

I'm Jihad Ftouny, born and raised Brazilian. I've got a Computer Science degree, and now, I'm proudly sporting a certification as an AI and Machine Learning Engineer, thanks to the team at Zaka AI. I've been tech-obsessed since forever, especially when it comes to computers.

I've played my first video-game at the ripe age of 3 during the SNES age, and got into computers at 6. One could say I have 20 years of experience! &#x1F605;

Over these two decades, I ventured into game/web development, photo/video editing, and even music creation. However, it was in the realms of AI and Game Development that I truly discovered my passion. I also enjoy the art of designing and coding websites, and I'm currently pursuing freelance projects in that domain.

### Zaka AI Certification

Before we jump into GPTector, let me talk a bit from where I got my AI skills. I can say with absolute confidence that if I had the choice, I wouldn't have chosen any other institution to learn from.

I selected Zaka AI as my guiding light and mentor throughout this learning journey because, right from our first interaction, their unwavering passion shone through. The coaching approach adopted by their instructors held immense significance for me, and I'm certain it did for many of my peers as well.

Their instruction goes beyond theory, they provide numerous hands-on exercises to put your knowledge to the test.

This program made me realize what I'm really capable of, and I'd recommend it to anyone who can code a bit and wants to make some difference in the world. The possibilities with AI are limitless!

# GPTector: An experiment with detecting short text generated by Open AI's ChatGPT

Project done by <a target="_blank" href="https://www.linkedin.com/in/ahmed-1eid/">Ahmed Eid</a> and <a target="_blank" href="https://www.linkedin.com/in/jihad-ftouny/">Jihad Ftouny</a>.

This project was an integral part of a research endeavor by the MindField team. They are pioneering lightweight solutions for identifying short texts generated by GPT, a task that presents its own unique set of challenges. The pursuit of such solutions aligns with the current trend in machine learning, as they significantly reduce carbon emissions.

Having that said, the goal while creating this ML solution was to advance research, and not come up with a perfect detector. We got impressive results nonetheless, and much can still be done with the right amount of data available.

I would like to extend my sincere appreciation to Omran Ayoub from MindField for his excellent guidance throughout the whole three weeks that expanded the project. I'm sure that without his instructions and insights we couldn't have gotten the excellent results we did.

## Why seek this solution?

One significant aspect of this challenge revolves around handling short text segments. With the prevalence of brief content in social media, reviews, and quick communications, there's a need to accurately analyze such content.

It's not uncommon to hear that a new business closed due to being bombarded with fake reviews on different platforms, or that someone responding to your social media messages is not an actual human (without explicitly announcing it). With such problems, and more arising each new day, solutions to identify AI generated text are most needed.

## What is GPTector and what does it to?

GPTector is as simple as it can get. You put your text there, press a button, and voil√†. A percentage result shows up on the screen telling you what are the chances of your text being AI or human generated.
<br>

![GPTector screenshot](/assets/images/gptector/gptector-screenshot.png)

# Step 1: It's All About That Data!

The dataset used to train our model was provided by MindField, consisting of a set of questions, with answers provided by either humans or ChatGPT. It is structured as a 11 columns (plus one for the ID) and a total of 46678 columns (divided equally between human and ChatGPT answers).

For the sake of simplicity, we will only visualize the table after we filtered the columns we used:
<br>

![Data visualization 1](/assets/images/gptector/data1.png)

- Here we can see the ID column repeated, due to each question having both ChatGPT and human answers.
- The source column represents the topic of the data or its source.
- There were initially five sources:
  - reddit_eli5 (explain like I'm 5)
  - open_qa
  - finance
  - medicine
  - wiki_csai (computer science AI)
- Label 0 stands for Human, while Label 1 stands for ChatGPT answers
- Words-amount and chars-amount are the counts of words and characters in the answer column

With the data explained, we can delve now into the process of dealing with it.

<p style='color: red; font-weight: bold;'>!! Disclaimer !! </p> <p style='color: red;'>Throughout our experiment, and at a point of no return, we realized that all the human answers from the reddit source contained separated punctuation. This was causing a huge bias for the model to predict human when punctuation was used. We opted to remove it completely and not use it since our time was short and we still had many steps ahead of us. For future experiments, we will clean it and take it into our model's training.</p>

### First steps, general analysis!

The first we did was to do an analysis of the data and get statistics summary of the numerical columns for each label:

```
# General summary
data.info()

# Summary statistics for label 0
label_0_summary = data[data['label'] == 0].describe()

# Summary statistics for label 1
label_1_summary = data[data['label'] == 1].describe()

print("Human Summary:")
print(label_0_summary)

print("\nGPT Summary:")
print(label_1_summary)
```

Results:

```
Human Summary:
        label  words-amount  chars-amount  readability_score
count  7210.0   7210.000000   7210.000000        7210.000000
mean      0.0    138.033148    813.141609          10.059390
std       0.0    140.925041    822.531879           4.365687
min       0.0      1.000000      5.000000          -3.100000
25%       0.0     45.250000    270.000000           7.100000
50%       0.0     92.000000    547.000000           9.300000
75%       0.0    183.000000   1079.000000          12.300000
max       0.0   1629.000000  10070.000000          46.800000

GPT Summary:
        label  words-amount  chars-amount  readability_score
count  7210.0   7210.000000   7210.000000        7210.000000
mean      1.0    185.511234   1119.759362          11.956269
std       0.0     68.250160    412.845608           2.764911
min       1.0      0.000000      0.000000         -15.700000
25%       1.0    143.000000    866.250000          10.400000
50%       1.0    189.000000   1144.000000          11.900000
75%       1.0    228.000000   1383.000000          13.600000
max       1.0    550.000000   3158.000000          45.000000
```

In the 'Human' labeled data, there's a wider variation in both word and character counts, with texts ranging from 1 to 1,657 words and 5 to 10,070 characters. Conversely, the 'GPT' labeled data is more concise, with entries between 0 to 549 words and 0 to 3,158 characters.

# Step 2: Let's Explore the Data! (EDA)

### Sentiment Analysis

We attempted to get information from sentiment analysis using the `TextBlob` library's sentiment polarity, however the sentiments of both Human and ChatGPT text was very similar in our dataset.

However, we did find something curious:
<br>

![Sentiment analysis graph](/../assets/images/gptector/sentiment-analysis.png)

The big bar at 0.00 represents that human text in our data has many true neutral occurrences, but in general the sentiment polarity is very similar between Human and ChatGPT text.

### Complexity Analysis

We had a shot at the Flesch-Kincaid readability grades using the `textstat` library, and from the graph below, we can see that the complexities between Human and ChatGPT text are very similar, with human text being slightly more complex:
<br>

![Complexity analysis graph](/../assets/images/gptector/complexity-analysis.png)

### Word Frequency

Now it's time to see how frequent words are used by humans and ChatGPT.

The following 2 graphs are without filtering the stopwords, thus they are the top occurences:
<br>

![Word Frequency analysis graph human](/../assets/images/gptector/word-frequency-human.png)
![Word Frequency analysis graph gpt](/../assets/images/gptector/word-frequency-gpt.png)

Now, filtering the stopwords:
<br>

![Word Frequency analysis graph human without stop words](/../assets/images/gptector/word-frequency-human-filtered.png)
![Word Frequency analysis graph gpt without stop words](/../assets/images/gptector/word-frequency-gpt-filtered.png)

We can see that the word choices are varied, and ChatGPT repeats its choices lot. Much more than that of human text. We did have some suspicions that it could be caused due to ChatGPT having longer texts, and this will be explored in future experiments.

### nGrams Frequency

Then we decided to see repetition in short sentences, with 3 and 5 words. For simplicity we will only display the results we obtained from the 3-Grams with 30 results:
<br>

![nGram graph human](/../assets/images/gptector/ngrams-human.png)
![nGram graph gpt](/../assets/images/gptector/ngrams-gpt.png)
Here we can see from the x-axis that GPT really likes to repeat certain sequences of words. Here we decided that GPT is actually very repetitive, even with the possibility of GPT having longer texts in our data as mentioned in the step above.

# Step 3: Done with Exploring? Let's Preprocess!

We first checked for outliers, since earlier during the Exploratory Data Analysis we saw that humans have sentences that reach over 1600 words while ChatGPT reached a maximum of 550.

Our goal is to ensure the model doesn't become biased towards recognizing human responses based solely on length. Also, we'll remove excessively short texts, especially those that are completely blank.

We will do that with a frequency-based system:

```
data['words-amount'] = data['answer'].apply(lambda x: len(str(x).split()))

# Calculate the frequency of each sequence length
sequence_length_freq = data['words-amount'].value_counts()

# Convert it to a dataframe and display it
sequence_length_freq_df = sequence_length_freq.sort_values(ascending=False).reset_index()
sequence_length_freq_df.columns = ['words amount', 'Frequency']
sequence_length_freq_df_sorted_by_length = sequence_length_freq_df.sort_values(by='words amount',ascending=False)

sequence_length_freq_df_sorted_by_length
```

We separated each occurence words amount and noted down how many times they occur in a frequency column:
<br>

![word count by frequency graph](/../assets/images/gptector/words-amount-freq-table.png)

Then we filtered the outliers out, based on having a frequency less than 9:

```
# Find word counts with a frequency of 8 or more
frequent_lengths = sequence_length_freq[sequence_length_freq > 8].index

# Create a mask for rows that need to be removed
remove_mask = ~data['words-amount'].isin(frequent_lengths)

# Get the questions corresponding to those rows
questions_to_remove = data[remove_mask]['question'].unique()

# Step 2: Remove rows corresponding to the identified questions

# Use the isin() method to filter rows that have the questions in the 'questions_to_remove' list
data_filtered = data[~data['question'].isin(questions_to_remove)]

# Display the filtered dataframe
data_filtered.head()
```

The code identifies and removes entries with word counts occurring less than 9 times, considered as outliers. Additionally, to maintain balance, it filters out the counterpart entries of the other class based on the same questions. This ensures both classes in the dataset remain balanced post-cleanup.

## Choose your destiny.

Or rather, we chose which path to take from here on to try and come up with a good model.

Since we were to reach good results with a lightweight model, we chose Sensitivity Analysis from the many suggested approaches by our mentor at MindField, `Omran Ayoub`.

Sensitivity Analysis is a valuable technique used in various fields, including data analysis and modeling, to assess the impact of varying input parameters on the output or model's performance. It helps to understand how sensitive a model or system is to changes in specific variables, which is crucial for decision-making, risk assessment, and optimizing models.

In our case, we are going to see how much data from outside the training set the model needs to have a good (or better) performance.

Since we already have the dataset divided into 4 sources/topics (remember that reddit was removed, so we ended up with one less), we are going to separate the data into these 4 topics.

```
Number of rows in openqa_df: 2364
Number of rows in wiki_df: 1566
Number of rows in finance_df: 7359
Number of rows in medicine_df: 2478
```

We created a function to perform `Feature Extraction` so we can use Word2Vec:

```
# Function to convert a document to a vector
def document_to_vector(doc, model):
    # Get the vector for each word in the document if it exists in the model's vocabulary
    vectors = [model.wv[word] for word in doc if word in model.wv.index_to_key]
    if vectors:
        return np.mean(vectors, axis=0)
    else:
        return np.zeros(model.vector_size)
```

# Step 4: Modeling

Having in mind we are going to perform Sensitivity Analysis, we combined 3 dataframes and left 1 for testing. We did this until all combinations were fulfilled. For simplicity, we will display the code of only one, but the results of all.

### Code: Wiki

```
# Combine the dataframes into a single training dataframe, leaving wiki for testing
dfs_to_train = [openqa_df, medicine_df, finance_df]
train_df = pd.concat(dfs_to_train)
```

```
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report


# Tokenize the sentences
sentences = train_df['answer'].str.split().tolist()

# Train a Word2Vec model
word2vec_model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)
word2vec_model.train(sentences, total_examples=len(sentences), epochs=5)

# Define X and y
X = train_df['answer'].str.split().apply(lambda x: document_to_vector(x, word2vec_model)).tolist()
y = train_df['label']

# Fit the model
clf = LogisticRegression(random_state=0).fit(X, y)
#define testing data
X_test = wiki_df['answer'].str.split().apply(lambda x: document_to_vector(x, word2vec_model)).tolist()
y_pred = clf.predict(X_test)
y_test = wiki_df['label']

print(classification_report(y_test, y_pred))
```

```
              precision    recall  f1-score   support

           0       0.92      0.44      0.59       727
           1       0.66      0.97      0.79       839

    accuracy                           0.72      1566
   macro avg       0.79      0.70      0.69      1566
weighted avg       0.78      0.72      0.70      1566
```

By training our model using three domains and testing on the Wiki domain, we achieved F1-score of 70%

Now, we will train the model iteratively for the sensitivity analysis:

```
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import f1_score
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression


# Split the data into 60% test and 40% train
retrain_data, remaining_data = train_test_split(wiki_df, test_size=0.60, random_state=42)

X_test = remaining_data['answer'].str.split().apply(lambda x: document_to_vector(x, word2vec_model)).tolist()
y_test = remaining_data['label']

chunk_size = int(0.05 * len(wiki_df))
chunks = [i for i in range(0, len(retrain_data) + 1, chunk_size)]

f1_scores = []

for start, end in zip(chunks[:-1], chunks[1:]):
    train_chunk = retrain_data.iloc[start:end]

    X_train = train_chunk['answer'].str.split().apply(lambda x: document_to_vector(x, word2vec_model)).tolist()
    y_train = train_chunk['label']

    clf.fit(X_train, y_train)

    y_pred = clf.predict(X_test)

    score = f1_score(y_test, y_pred)
    f1_scores.append(score)
    print(classification_report(y_test, y_pred))

# Plotting the results
plt.figure(figsize=(10, 6))
plt.plot([0] + [5] + [10] + [15] + [20] + [25] + [30]+ [35]+ [40], [0.70] + f1_scores, '-o', color='blue', markerfacecolor='red', markersize=8)
plt.title("F1 Score vs. Training Data Size(Wiki)")
plt.xlabel("Percentage of Training Data")
plt.ylabel("F1 Score")
plt.grid(True)
plt.xticks([0, 5, 10, 15, 20,25,30,35,40])
plt.show()
```

Here are the results of Wiki, and all others (note that for medicine the results were so high initially we decided to keep it at 1 iteration):

![Wiki Short Text Sensitivity Analysis Graph](/../assets/images/gptector/wiki-sens-analyis.png)
![medicine Short Text Sensitivity Analysis Graph](/../assets/images/gptector/medicine-sens-analysis.png)
![openqa Short Text Sensitivity Analysis Graph](/../assets/images/gptector/openqa-sens-analysis.png)
![finance Short Text Sensitivity Analysis Graph](/../assets/images/gptector/finance-sens-analysis.png)

The results improved drastically with as litle as 5 percent in most cases.

## Short-Text Improvements

For enhanced performance with shorter texts, we trimmed the answers to fall within a range of 7 to 30 words. We aimed for the nearest full stop for natural sentence breaks, but if the answers didn't reach a full stop, they were trimmed down to a minimum of 7 words.

```
def trim_text(text):
    words = text.split()

    if len(words) > 30:
        # Trim to 25 words initially
        trimmed_text = ' '.join(words[:30])

        # Check if the last character is a full stop
        if trimmed_text[-1] != ".":
            # If not, keep removing words until a full stop is reached
            while trimmed_text[-1] != "." and len(trimmed_text.split()) > 7:
                words = trimmed_text.split()
                trimmed_text = ' '.join(words[:-1])

        return trimmed_text
    else:
        return text

data_filtered['short-answer'] = data_filtered ['answer'].apply(trim_text)
```

We performed the same modeling as above, but with this newly generated 'short-answer' column, and got the following results:

![Wiki Sensitivity Analysis Graph](/../assets/images/gptector/wiki-short-sens-analyis.png)
![medicine Sensitivity Analysis Graph](/../assets/images/gptector/medicine-short-sens-analysis.png)
![openqa Sensitivity Analysis Graph](/../assets/images/gptector/openqa-short-sens-analysis.png)
![finance Sensitivity Analysis Graph](/../assets/images/gptector/finance-short-sens-analysis.png)

Here we can clearly see the challenge that is identifying short text generated by ChatGPT. We couldn't see as much improvement as before, and in the case of OpenQA, no improvement at all!

Now, the nature of OpenQA data is very general and broad, and could be the cause for this lack of improvement. Thus, sensitivity analysis is arguably very useful for this use-case.

We also attempted to do sensitivity analysis with the `Random Forest` algorithm. Unfortunately, we got the same F1-Scores, and even worse with some of the topics.

## The Chosen Model

A simple model such as Logistic Regression proved to be useful with longer texts, and we wanted to see if we could get better results for shorter texts with a different model.

We attempted with XGBoost successfully, but at the cost of using much more data for the sensitivity training:

![Wiki XGBoost Short Text Sensitivity Analysis Graph](/../assets/images/gptector/wiki-xg-short-sens-analyis.png)
![medicine XGBoost Short Text Sensitivity Analysis Graph](/../assets/images/gptector/medicine-xg-short-sens-analysis.png)
![openqa XGBoost Short Text Sensitivity Analysis Graph](/../assets/images/gptector/openqa-xg-short-sens-analysis.png)
![finance XGBoost Short Text Sensitivity Analysis Graph](/../assets/images/gptector/finance-xg-short-sens-analysis.png)

# Final Decisions and Future Attempts

For our web-app, we decided to use the Logistic Regression model when reading texts over 30 words, and the XGBoost model for the other case.

Some considerations that can be taken for the improvement of this model:

- Use the reddit data after repairing it.
- Train on more and more topics, generalizing the model successfully.
- Use Explainable AI techniques to understand how the model is doing its decision.
- Try different advanced techniques not yet explored for this use-case.

## Try our GPTector model!

Please note that this model is hosted using free services and the website might take some time to load, usually between 30 seconds and 1 minute and a half.

<a target="_blank" href="https://gptector-flask.onrender.com/">Visit GPTector</a>
